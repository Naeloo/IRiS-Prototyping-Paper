%
% FH Technikum Wien
% !TEX encoding = UTF-8 Unicode
%
% Erstellung von Master- und Bachelorarbeiten an der FH Technikum Wien mit Hilfe von LaTeX und der Klasse TWBOOK
%
% Um ein eigenes Dokument zu erstellen, müssen Sie folgendes ergänzen:
% 1) Mit \documentclass[..] einstellen: Master- oder Bachelorarbeit, Studiengang und Sprache
% 2) Mit \newcommand{\FHTWCitationType}.. Zitierstandard festlegen (wird in der Regel vom Studiengang vorgegeben - bitte erfragen)
% 3) Deckblatt, Kurzfassung, etc. ausfüllen
% 4) und die Arbeit schreiben (die verwendeten Literaturquellen in Literatur.bib eintragen)
%
% Getestet mit TeXstudio mit Zeichenkodierung ISO-8859-1 (=ansinew/latin1) und MikTex unter Windows
% Zu beachten ist, dass die Kodierung der Datei mit der Kodierung des paketes inputenc zusammen passt!
% Die Kodierung der Datei twbook.cls MUSS ANSI betragen!
% Bei der Verwendung von UTF8 muss dnicht nur die Kodierung des Dokuments auf UTF8 gestellt sein, sondern auch die des BibTex-Files!
%
% Bugreports und Feedback bitte per E-Mail an latex@technikum-wien.at
%
% Versionen
% *) V0.7: 9.1.2015, RO: Modeline angepasst und verschoben
% *) V0.6: 10.10.2014, RO: Weitere Anpassung an die UK
% *) V0.5: 8.8.2014, WK: Literaturquellen überarbeitet und angepasst
% *) V0.4: 4.8.2014, WK: Initalversion in SVN eingespielt
%
\documentclass[BSA,Bachelor,english]{twbook}%\documentclass[Bachelor,BMR,german]{twbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{cmap}

%
% Bitte in der folgenden Zeile den Zitierstandard festlegen
\newcommand{\FHTWCitationType}{IEEE} % IEEE oder HARVARD möglich - wenn Sie zwischen IEEE und HARVARD wechseln, bitte die temorären Dateien (aux, bbl, ...) löschen
%
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{\usepackage{harvard}}{\usepackage{bibgerm}}

% Definition Code-Listings Formatierung:
\usepackage[final]{listings}
\lstset{captionpos=b, numberbychapter=false,caption=\lstname,frame=single, numbers=left, stepnumber=1, numbersep=2pt, xleftmargin=15pt, framexleftmargin=15pt, numberstyle=\tiny, tabsize=3, columns=fixed, basicstyle={\fontfamily{pcr}\selectfont\footnotesize}, keywordstyle=\bfseries, commentstyle={\color[gray]{0.33}\itshape}, stringstyle=\color[gray]{0.25}, breaklines, breakatwhitespace, breakautoindent}
\lstloadlanguages{[ANSI]C, C++, [gnu]make, gnuplot, Matlab}

%Formatieren des Quellcodeverzeichnisses
\makeatletter
% Setzen der Bezeichnungen für das Quellcodeverzeichnis/Abkürzungsverzeichnis in Abhängigkeit von der eingestellten Sprache
\providecommand\listacroname{}
\@ifclasswith{twbook}{english}
{%
    \renewcommand\lstlistingname{Code}
    \renewcommand\lstlistlistingname{List of Code}
    \renewcommand\listacroname{List of Abbreviations}
}{%
    \renewcommand\lstlistingname{Quellcode}
    \renewcommand\lstlistlistingname{Quellcodeverzeichnis}
    \renewcommand\listacroname{Abkürzungsverzeichnis}
}
% Wenn die Option listof=entryprefix gewählt wurde, Definition des Entyprefixes für das Quellcodeverzeichnis. Definition des Macros listoflolentryname analog zu listoflofentryname und listoflotentryname der KOMA-Klasse
\@ifclasswith{scrbook}{listof=entryprefix}
{%
    \newcommand\listoflolentryname\lstlistingname
}{%
}
\makeatother
\newcommand{\listofcode}{\phantomsection\lstlistoflistings}

% Die nachfolgenden Pakete stellen sonst nicht benötigte Features zur Verfügung
\usepackage{blindtext}

%
% Einträge für Deckblatt, Kurzfassung, etc.
%
\title{Development of a hand gesture recognition plugin for the AsTeRICS framework}
\author{Leonhard Hauptfeld}
\studentnumber{1510768031}
%\author{Titel Vorname Name, Titel\and{}Titel Vorname Name, Titel}
%\studentnumber{XXXXXXXXXXXXXXX\and{}XXXXXXXXXXXXXXX}
\supervisor{Ing. Martin Deinhofer, MSc}
%\supervisor[Begutachter]{Titel Vorname Name, Titel}
%\supervisor[Begutachterin]{Titel Vorname Name, Titel}
%\secondsupervisor{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachter]{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachterinnen]{Titel Vorname Name, Titel}
\place{Vienna}
\kurzfassung{
Handgesten sind ein wichtiger Teil der menschlichen Körpersprache. Ihr Nutzen umfasst ein weites Feld, von Grundfunktionen wie der Assistenz von linguistischer Kommunikation, bis zu einer komplett eigenen Sprache für taube Menschen. Demnach ist die Erkennung von Handgesten ein großer Teil in dem Feld der "Computer Vision". Sie bilden oft einen wichtigen Teil von Softwarelösungen im Bereich der Assistiven Technologien (AT). Trotzdem ist mit Gestenerkennung oft eine steile Lernkurve und eine hohe Eintrittsbarriere verbunden. Sie erfordert komplexe Algorithmen, implementiert in schwierigen hardwarenahen Programmiersprachen, und normale Computer-Webcams sind den Anforderungen oft nicht gewachsen. Lösungen für diese Probleme existieren teilweise in Form von AT-Toolkits (Frameworks für die schnelle Implementierung von Lösungen) und 3D-Kameras mit Tiefensensoren. Diese Arbeit beschäftigt sich mit der Implementierung eines Handgestenerkennungs-Plugins für das AT-Toolkit "Assistive Technology Rapid Integration \& Construction Set" (AsTeRICS). Die Software verwendet dafür eine 3D-Kamera ausgestattet mit Intel RealSense-Technologie. Die Funktionalität beschränkt sich auf die Erkennung einer Handform und eines Zählens der ausgestreckten Finger allerdings wird das Plugin offen für Erweiterungen implementiert. In der Einführung wird ein kurzer Überblick über State-of-the-art-Technologien passend für die Implementierung gegeben. Anschließend wird eine Methode für die Konzeptionierung und Implementierung der Software entwickelt und angewendet. Das resultierende Plugin wird im Rahmen einer kleinen Test-AT-Lösung auf die Funktionalität hin evaluiert. Die angewendete Implementierungsmethode wird analysiert und Verbesserungsmöglichkeiten sowie aufgetretene Fehler in der Konzeptionierung werden in Ratschläge für weitere Plugin-Entwicklung intepretiert. Zuletzt werden Zukunftsmöglichkeiten für die Software wie z.B. weitere Entwicklung aufgelistet.
}
\schlagworte{Assistive, Technologie, Software, Integration, Einbindung, Bewegungserkennung, Computer Vision}
\outline{
Hand gestures are an important part of human body language. Their uses range from assisting verbal expression to a complete form of communication for the deaf. As a result, recognition of hand gestures by a machine is a large segment of computer vision research and often an important part of assistive technology (AT) solutions. However, there is a steep learning curve and high barrier of entry associated with gesture recognition. The processing requires complex algorithms programmed in difficult low-level programming languages and normal color cameras such as webcams are often ill suited for the task. Solutions for these problems partly exist in the form of AT-Toolkits that provide frameworks for quick implementation of assistive solutions and advanced computer cameras with the ability to not only capture images but also depth information. This paper pursues the implementation of a hand gesture recognition plugin for the AT-Toolkit known as "Assistive Technology Rapid Integration \& Construction Set" (AsTeRICS) utilizing a depth camera equipped with Intel RealSense technology. The functionality of this plugin is limited to recognizing the shape of a hand and detecting the number of extended fingers on it. The software is built with expandability in mind. In the introduction, a short overview of state-of-the-art technologies suited for this task is given. A method for approaching and executing the implementation is created. This method is applied and the resulting plugin is evaluated for its functionality in an example AT solution. The method of implementation is discussed and flaws uncovered in it are interpreted into advice for approaching further plugin development. Finally, the future prospects for the developed software are listed.
}
\keywords{assistive, technology, software, integration, motion recognition, computer vision}
\acknowledgements{\blindtext}

\begin{document}

%Festlegungen für den HARVARD-Zitierstandard
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{
\bibliographystyle{Harvard_FHTW_MR}%Zitierstandard FH Technikum Wien, Studiengang Mechatronik/Robotik, Version 1.2e
\citationstyle{dcu}%Correct citation-style (Harvardand, ";" between citations, "," between author and year)
\citationmode{abbr}%use "et al." with first citation
\iflanguage{ngerman}{
    %Deutsch Neue Rechtschreibung
    \newcommand{\citepic}[1]{(Quelle: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Quelle: \protect\cite{#1}, S. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Quelle: modifiziert "ubernommen aus \protect\cite{#1}, S. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Verf{\"u}gbar unter:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Verf{\"u}gbar bei:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Zugang am}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{S.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{S.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{K.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{ausschließlich Abstract}
    \newcommand{\edition}{. Auflage}%Angabe der Auflage
}{
\iflanguage{german}{
    %Deutsch
    \newcommand{\citepic}[1]{(Quelle: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Quelle: \protect\cite{#1}, S. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Quelle: modifiziert "ubernommen aus \protect\cite{#1}, S. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Verf{\"u}gbar unter:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Verf{\"u}gbar bei:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Zugang am}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{S.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{S.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{K.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{ausschließlich Abstract}
    \newcommand{\edition}{. Auflage}%Angabe der Auflage
}{
    %Englisch
    \newcommand{\citepic}[1]{(Source: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Source: \protect\cite{#1}, p. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Source: taken with modification from \protect\cite{#1}, p. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Available at:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Available through:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Accessed}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{p.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{pp.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{Ch.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{Abstract only}
    \newcommand{\edition}{~edition}%Edition -> note, that you have to write "edition = {2nd},"!
}}}

\maketitle

%
% Start des Haupttextes
%
\chapter{Introduction}



\section{State of the art}
\subsection{AT-Frameworks}
AsTeRICS is a framework for building AT-Solutions that is based entirely on different plug-ins and their interaction. Possible plug-in types are sensors and actors. It is written in Java, utilizing native C++ libraries where necessary or advantageous.

\subsubsection{OSGi}
The runtime application is implemented as a framework utilizing the OSGi platform concept by the OSGi Alliance. It therefore provides a middleware layer to all the sensor and actor plugins that they can use to connect to the main application. These plugins represent the component model and provide their services to the main application over the middleware library.
\subsection{Computer Vision}

Various computer vision libraries for research exist, but OpenComputerVision (OpenCV) has established itself as the dominant library used for most computer vision applications, becoming almost synonymous for the technology. It contains a vast selection of structures and algorithms for loading, storing and transforming computer vision relevant data.

OpenCVs core is implemented in C, making it a very resource-efficient library. It is available for every major operating system and processor architecture. Official bindings with documentation exist for C++, Python and Java with many more unofficial ones for almost every other important programming language.

\subsection{Programming languages}
\subsubsection{Java}
Java\cite{JAVA} is a programming language invented by Sun Microsystems, now owned by the Oracle Corporation, which also supports its most prominent implementation. It compiles into non-native Java Bytecode that runs on a Java Virtual Machine (JVM). As a result, any compiled Java Code can run on any hardware that runs such a JVM, making Java almost entirely platform independent.

It features a very strict object oriented programming model with, in comparison with similar languages like C\#, little convenience features, making it one of the more challenging programming languages for development. Additionally, the platform independence of the Java Virtual Machine means that Java programs are quite more resource intensive than those of natively executed languages\cite{SPD_COMPARISON}.

Java can be coupled to native Machine Code written in C or C++, giving it additional versatility at the cost of complete platform independence. As such, bindings exist for many popular C/C++ libraries, including OpenCV. The OpenCV binding to Java is officially supported by the core OpenCV team.
\subsubsection{C++}
C++ is a programming language defined and standardized by the International Organization for Standardization (ISO)\cite{CPP}. There are multiple implementations of this standard (each differing slightly), the most prominent ones being as part of the free GNU Compiler Collection project and Visual C++ by Microsoft.
\subsubsection{Python}
Python\cite{PYTH} is an interpreted programming language with a simple syntax, partially derived the language "ABC", a simple language with the original purpose of teaching children programming. Despite its simple appearance it is closely tied in with C and C++, with many libraries being direct ports from these programming languages. Prominent examples of this include OpenCV (supported by the core development team), QT and many more. Furthermore, it also has a number of libraries for scientific computing available exclusively to it, such as the hugely popular numpy.

Being an interpreted language, Python is not as fast as most compiled languages. However, single methods of the C/C++ libraries will perform almost as fast as their native counterparts, since they just refer back to their native assemblies. As such, program logic is mostly implemented in Python, while performance critical algorithms are written in C/C++.

The versatility coupled with the easy syntax makes Python a very attractive technology to use for prototyping and scientific computation.

\subsection{Data exchange formats}

\subsubsection{JSON}

JavaScript Object Notation is a text-based data exchange format based on the object definition syntax in the programming language JavaScript. It is defined by the Standards RFC8259\cite{RFC8259} and ECMA-404\cite{ECMA404}. The simple syntax with only 4 object types (string, number, array, object) makes it easily human-readable while the same object syntax is native to a few programming languages (like JavaScript and Python) leading to fast and easy processing.

JSON data is most commonly transmitted via the Hypertext Transfer Protocol (HTTP) from a web service utlizing the Representational State Transfer (REST) model. The exact format of JSON and the means of exchange are not standardized, leading to vastly different types of usage and often completely incompatible web services merely able to parse the data, not process it. No acknowledged standards exist. An attempt at providing standardized JSON schemas has been made through the JSON-Schema project\cite{JSON_SCHEMA}.

\subsubsection{XML}

The Extensible Markup Language is also a text-based data exchange format specified\cite{XML} by the World Wide Web Consortium (W3C). It uses a system of tags and attributes to represent hierarchical data.

Its wide use in commercial applications gives it a large advantage over JSON when it comes to standardization. There is an official XML Schema recommendation and a Document Type Definition (DTD) standard by the W3C. Furthermore, web services can be described in the Web Services Description Language (WSDL), which is a derivative of XML. A standardized way of transforming XML exists in the XML Stylesheet (XSLT) standard. XLST provides the means to transform any XML document from one web service into a completely different XML arrangement for use with another.

\newpage
\chapter{Integration method}

\section{Analyzing target software}
This proposed method starts with an analysis of the technology used to build both the target software and the technology to be integrated at different tiers.

At the core level, this is the programming language used to implement it. Integration is naturally easiest when these are the same or very similar. A method used in one program to encode an output is guaranteed to have an accompanying decoding method in the other program. 

A level above this lie any direct extensibility interfaces. This includes all methods of directly attaching your own code to the target software. Examples for this are loading of shared libraries (dll, so, etc.) or Java JAR files.

The last level to analyze is any networking components. The application might expose a REST API via a local HTTP server or other information via a custom protocol and a local socket.

These levels are sorted from lowest to highest level interfacing from a technical standpoint. This is by no means a way to rank their usability, as the other software might have better support for the networking components or another layer might have shortcomings in a certain technology that makes it unable to support usage for assistive technology. For example, a JavaScript extension for an application is much more suited for interfacing via the networking layer than ones on a lower level. However, if the target software does not implement some kind of security protocol on that layer, it might not be suited for exchanging medical data.

Those are some reasons and an example why the integration of software, especially complex technology, might not be extremely straightforward and why it is important to choose the right technology.

\section{Choosing the right technology}

The simplest proposed methodology is to find the lowest level interface that the software to be integrated supports at its own lowest level, and use these end points to integrate the two pieces of software. This crude approach might be well suited if the two pieces of target software are very similar in nature. A directly extensible C++ program would not use a web layer to extend another C++ application when it can be directly connected via shared library plugin extension and the target software supports that method.

Another point to consider is the difference of closeness to hardware between the target software and the software to be integrated. If the target software is of a higher level it us usually easier to extend it with a lower level language because high level programming languages are built on low level technology. A good example of this is the integration of native C++ code into Java applications using the Java Native Interface. With this in mind, keeping the level of the software to be integrated at a higher level is more desirable, due to high level programming languages usually being easier to develop with and the loss of high level functionality when coupled with low level technology, such as the loss of easy platform independence when using the Java Native Interface.

The existing technology available to the programming language to be integrated is also important. While it is not usually possible to directly change things like dependencies about the target software, the software to be integrated can often be quite freely extended.

\section{Choosing the attachment point}

Having chosen layer and technology through the previous two methods, choosing an attachment point should now be the simple matter of looking at extendability options on the chosen layer and choosing the option best suited to the selected technology. Usually, this will only result in only one way to reasonably combine all the pieces.

If there are multiple options, the use case for the plugin should be evaluated and the best option chosen. Generally, the more independent from the target software the attachment point is, the more versatile and maintainable the resulting software will be. This is because a mostly self-contained plugin can more easily be integrated into a different system than something deeply intertwined with the target software.

This will also result in more maintainability because more of the source code is independent from the target and as such will not suffer from any failures that the target software might develop. Developers of the extendability interface will always seek to provide maximum compatibility, shifting workload from the plugin developer to the core development team who usually have a better understanding of the target software's internal processes.


\chapter{Plugin development method}


\section{User Interface}

Choosing where to have a user configure the application to be integrated is not always straightforward. Sometimes the target software provides a means to integrate a user interface. This approach is almost always the most desirable one, as it provides the most streamlined experience. Sometimes, the target software does not provide the means for this or they are not sufficient for the task. In this case, the software to be integrated has to provide some own means of configuration and / or interface. The easiest solution comes in the form of a configuration file to be edited by the user. More functionality like a graphical user interface can be added as well to the degree that the guidelines of the target software and the possibilities in the source software allow.

\section{Development Platforms \& Tools}

% bla bla build system bla bla ide written in native language bla bla
The platform and software used to develop the add-in should ideally be the same as the one used to develop the target software. This is a problematic approach when the original development software is outdated or not available on the platform. Familiarity with certain software like a particular Integrated Development Environment plays a role too because it can dramatically affect the speed of development. The choosing of development software should take all these factors into account and result in the best compromise between these factors.

\chapter{Implementation}

\section{Hardware}

% TODO: Requirements link?
The 

\section{Software}
%TODO: List versions
For C++ development, the IDE "CLion" by IntelliJ was chosen, together with the integrated CMake build system. There was no particular requirement for features of the C++ IDE because the native library is mostly independent from AsTeRICS and its plugin system. The libraries used in the native C++ support library were OpenCV, librealsense and the Java Native Interface due to their versatility and ability to integrate into one another.

For Java development, the IDE "IDEA" by IntelliJ was selected due to familiarity with it. The build system of the ASTeRICS framework is Ant. IDEA supports this build system and the integration of Ant into it was set up for the build process. The IDE also provides support for the OSGi framework. These factors meant that the development environment could be used for AsTeRICS plugin development even if the AT-Toolkit wasn't originally developed using it. 

The only external dependencies of the Java program is the ASTeRICS middleware and the native library developed for the recognition.

\section{Conceptualization}

The methods described earlier are now applied to the AsTeRICS framework to find a suitable layer to integrate the plugin into. At the core, AsTeRICS is written in Java. To provide support for extensibility through plugins on the layer above, it is written as an OSGi middleware. Finally, AsTeRICS can interact with network components through special plugins such as the WebSocket, used to connect to the AsTeRICS Runtime, for example via JavaScript in a browser.

All AsTeRICS components are plugins written in Java using the OSGi middleware layer provided by AsTeRICS. It is possible to implement additional functionality by using the JavaScript WebSocket from a browser window. For gesture recognition, JavaScript is too inefficient and opening a seperate browser window is a nuisance. Since AsTeRICS is open source, it would also be possible to directly implement the functionality into the software itself. However, the gesture recognition does not need functionality not available to plugins. For these reasons, the implementation of the gesture recognition plugin was to use the OSGi plugin interface.

Initially, the chosen programming language for the plugin was to be purely Java. AsTeRICS provides the JavaCV Java bindings for OpenCV. These are not the official Java bindings from OpenCV, but they include a Java version of the Intel "librealsense" library used for communication with the camera. After some prototyping, it was discovered that this approach had several severe drawbacks:

\begin{itemize}
	\item The main Java OpenCV binding was not the official OpenCV Java binding and as such may become outdated or suffer other flaws
	\item The Java "librealsense" binding was outdated and no Windows version was available, making it impossible to run the plugin on Windows
\end{itemize}

Consequentially the Java prototype was scrapped and the functionality replaced with an implementation using OpenCV in one of its and librealsense's native environments, the programming language C++. The functionality in this library would be coupled to the Java plugin using the Java Native Interface. This solution had the advantage of good library support and superb performance. The only drawback was the additional complexity of the Native Interface and the difficulty of cross-platform programming on a low level language like C++, resulting in more time spent setting up the toolchain.

\section{Implementation of Handtracking}
\subsection{RealSense and OpenCV}
The open source library "librealsense" by Intel used provides an easy means of accessing the RealSense camera directly from C++ code. A pipeline object is created and used in a loop to get the newest image data. For this project, only data from the camera's depth sensor is needed. The newest frame of depth data is received every loop iteration and converted to a standard OpenCV Matrix ("Mat") using a built-in librealsense method.

\begin{lstlisting}[language=C++,name={RealSense image capture and conversion},label={rs:loop:1}]

[...]

// Declare RealSense pipeline, encapsulating the actual device and sensors
rs2::pipeline pipe;
// Start streaming with default recommended configuration
pipe.start();

isRecognizing = true;
int previousFingers = 0;

using namespace cv;
while (gesture_visualizer.is_open() && isRecognizing)
{
	rs2::frameset data = pipe.wait_for_frames(); // Wait for next set of frames from the camera
	rs2::frame depth = color_map(data.get_depth_frame());
	//rs2::frame depth = data.get_depth_frame();
	
	// Query frame size (width and height)
	const int w = depth.as<rs2::video_frame>().get_width();
	const int h = depth.as<rs2::video_frame>().get_height();
	
	// Create OpenCV matrix of size (w,h) from the colorized depth data
	Mat image(Size(w, h), CV_8UC3, (void*)depth.get_data(), Mat::AUTO_STEP);
	[...]    
}
\end{lstlisting}

The default capture mode of the depth sensor outputs a BGR (Blue, Green and Red color components) image in 8 bit depth, as indicated by the 8-bit depth 3 channel ("CV\_8UC3") format of the matrix. For easier processing, a grayscale image is more suited. Therefore, a color scheme is set using a "colorizer" class built into the librealsense library.

\begin{lstlisting}[language=C++,name={RealSense image colorizer},label={rs:init:1}]
// Declare depth colorizer for pretty visualization of depth data
rs2::colorizer color_map;
// Use color scheme option 2 (grayscale, distant black, close white)
color_map.set_option(RS2_OPTION_COLOR_SCHEME, 2);
\end{lstlisting}

The resulting captured image in OpenCV Matrix format is then passed to the library's own "recognizer" class for further processing.

\subsection{Gesture recognition}

The "recognizer" class is where all of the image segmentation and hand detection occurs. Its method "get\_hand\_model" takes the Matrix depth frame and outputs a "hand\_model" structure, containing all the necessary info about how the hand was detected and, as a final result, how many fingers were detected as extended. The structure is as follows:

\begin{lstlisting}[language=C++,name={Hand Model Structure},label={rs:hand_model:1}]
struct hand_model{
	int num_fingers;
	std::vector<cv::Point> hand_contour;
	std::vector<int> hand_hull_indexes;
	std::vector<cv::Vec4i> hand_defects;
	std::vector<int> finger_defects_indexes;
	cv::Mat display_frame;
};
\end{lstlisting}

\newpage
The following steps are then applied to get the hand model from the passed Matrix:

\begin{labeling}{alligator}
	\item [\textbf{Conversion}] The grayscale BGR matrix is converted to a single grayscale channel
	\item [\textbf{Segmentation}] The arm is segmented from background noise. This is achieved by getting the median value of a small center region, and then removing all depth data within a certain threshold of that median. Gaps caused by an insufficient threshold are filled in and everything is colored gray. The segment of the image touching the middle (the hand) is then filled white and all remaining gray background noise is removed. The result is an image with a white hand contour in the middle.
	\item [\textbf{Defect Recognition}] Contours are drawn around the white segments in the frame. The contour with the biggest area is selected as the probable shape of the hand. It is smoothed to remove noise and a hull is drawn around it. The hull is a contour drawn around the outside of the hand, not accounting for "valleys" in the actual contour. This hull is then analyzed for convexity defects, which represent those valleys.
	\item [\textbf{Hand Processing}] The convexity defects are analyzed for the angle of the "valley". If it is smaller than a threshold, it is counted as an extended finger. Otherwise, it is discarded.
\end{labeling}

%TODO: Put source of steps here (packtpub)

All the above mentioned processes can be found in the "recognizer.cpp" class file, split into appropriately named methods. Comments in those methods describe the functionality in greater detail.

\subsection{Tracking and Information Window}

Sensible additional functionality for hand tracking software is some kind of user interface. This is because the person in front of the computer sometimes requires feedback on what the camera is seeing and how it is interpreting the data, so they can correct any malfunctions in the detection by re-aligning their hand. The simplest form of interface for this application would be a small window showing the video feed from the camera and a representation of the tracking model, and this was implemented using OpenCV's own HighGUI library. This library is only meant for debugging purposes and caused some issues when called from C++ code due to threading issues.

\subsection{Independent testing of the native library}

For faster testing and therefore faster development of the native library, a small executable program was developed that called the loop function of the library to run it for testing purposes. The loop function has to take into account if it has been given the Java environment reference variable. If it hasn't, the library was not called from a Java Native Interface context and therefore operates in testing modes, executing no native interface code.

\section{Integration into framework}

\subsection{Anatomy of an AsTeRICS plugin}

AsTeRICS plugins are located in folders named after their last two package names. This is usually results in a "<componentType>.<name>" format. In this folder, the required files are:

\begin{itemize}
	\item "build.xml" file for the ant build system, specifying the build options for the plugin
	\item "bundle\_descriptor.xml" file located in the "src/main/resources folder", describing the AsTeRICS plugin characteristics such as input ports, output ports, etc.
	\item Standard Java Manifest file "MANIFEST.MF" located in the "src/main/resources/META-INF" folder
	\item Main Java source file in the "src/main/java" folder, located within the appropriate package directory
\end{itemize}

The main plugin class, in this case called "RealSenseGesturesInstance", should have a package name that starts with "eu.asterics.component". The last two parts of the package name are the component type, e.g. "sensor", and a descriptive component package name, e.g. "realsensegestures". These last two parts of the package name should also be the same as the main directory name for the plugin.

\subsection{Plugin Creation}

To ease plugin creation, AsTeRICS comes with a Plugin Creation Wizard. This is a tool that provides a GUI for setting all plugin properties and then generates an empty "stub" plugin with the given configuration in the correct directory.

This tool is a Windows executable called "AsTeRICS\_PluginCreationWizard.exe" located in the "bin/ACS/tools" directory of the main AsTeRICS installation.

It is possible to use it within Linux using the "wine" emulator application. This was tested and working using "wine" version 2.4 under elementaryOS 0.4.1, an operating system closely related to Ubuntu 16.04 LTS "Xenial".

%TODO: Screenshot of wizard under "wine"

\subsection{Plugin Configuration and Options}

AsTeRICS plugins can have input ports where they can receive data and output ports where they can return data. Additionally, AsTeRICS supports an event system within which plugins can fire events to other plugins using "Event Trigger Ports" and receive said events from other plugins through "Event Listener Ports". Furthermore, plugins can be configured via a set of defined properties. All of the ports and properties have fixed primitive data types.

What follows is a short overview of the most important configuration options of an AsTeRICS plugin:
%TODO: Asterics Plugin Config Description
\begin{table}[]
	\centering
	\caption{AsTeRICS Plugin Configuration}
	\label{plugin-config}
\begin{tabular}{|l|l|l|}
	\hline
	Option Name          & Description & Configured Value \\ \hline
	Plugin Name          & Test        & 123              \\ \hline
	Type                 & Test        & 345              \\ \hline
	Subcategory          & Test        & 456              \\ \hline
	Input Ports          & Test        & 567              \\ \hline
	Output Ports         & Test        & 678              \\ \hline
	Event Listener Ports & Test        & 789              \\ \hline
	Properties           & TEst        & 8910             \\ \hline
\end{tabular}
\end{table}

\subsection{Main Plugin Class}

The main class of the AsTeRICS RealSense gesture recognition plugin is called "RealSenseGesturesInstance" and extends the abstract class "AbstractRuntimeComponentInstance" provided by the AsTeRICS middleware layer. This forces it to implement functionality used by AsTeRICS to retrieve metadata and output by the plugin as well as methods to receive data sent to it by the Toolkit.

\begin{lstlisting}[language=Java,name={Main plugin class and member variables},label={rs:javaplugin:1}]
public class RealSenseGesturesInstance extends AbstractRuntimeComponentInstance
{
	final IRuntimeOutputPort opDetected = new DefaultRuntimeOutputPort();
	final IRuntimeOutputPort opExtended = new DefaultRuntimeOutputPort();
	[...]
	private RealSenseNativeConnector native_conn;
	[...]
\end{lstlisting}

A "stub" class with basic empty implementations was generated by the Plugin Creation Wizard. The "Runtime Output Port" classes provide functionality for outputting data from the plugin. Two standard output ports were used here because that is the plugin configuration and no advanced functionality for the ports was needed. The default ports provide simple methods for outputting data in bit form.

The rest of the main plugin class functionality is left to default. Most of the workload is delegated to the class managing the connection to the native support library, "RealSenseNativeConnector", described in the chapter below. This class was created to keep the functionality in the component more compartmentalized. The main class methods are restricted to interaction with the AsTeRICS middleware and the connector class.

\subsection{Native Connector Class}

\subsubsection{Interaction with native code}

The class "RealSenseNativeConnector" implements the method called by the native library through the Java Native Interface. It is used for relaying the number of detected extended fingers to the running Java application.

\begin{lstlisting}[language=Java,name={Java Native Interface callback method},label={rs:javaplugin:3}]
// Callback called from C++ code when extended finger numbers change
public void fingerNumberChanged(int fingersExtended){
	if(transmitting) {
		AstericsModelExecutionThreadPool.instance.execute(new PassFingersRunnable(fingersExtended));
	}
}
\end{lstlisting}

This method, also known as a "Callback", is called on every iteration of the native recognition loop, running inside a separate thread within Java. As a parameter it is given the number of currently detected extended fingers. It confirms that the AsTeRICS model is not currently paused through the "transmitting" member variable of the connector class and, on fulfillment of that condition, passes the given number to the main AsTeRICS model execution thread via a class that extends the Java "Runnable" class. This has to be done because AsTeRICS does not accept output of values from a separate thread. The "Runnable" forwards these values to the output port in the main plugin class.

An alternative for passing data from the native library to the Java code was considered. The information can be passed between the environments by using a local networking socket on the computer. However, this would result in additional implementation overhead, as the format for the data exchange needs to be defined and implemented on both sides. It is also not as fast as the callback method, although due to the small amount of information being passed this was not really a considered factor. The approach was discarded because of the additional time it would have taken to implement and the little benefits it would have brought.

\begin{lstlisting}[language=Java,name={Forwarding runnable},label={rs:javaplugin:5}]
public class PassFingersRunnable implements Runnable {
	int extended;
	public PassFingersRunnable(int fingers_extended) {
		this.extended = fingers_extended;
	}
	
	@Override
	public void run() {
		parent.opExtended.sendData(ConversionUtils.intToBytes(extended));
	}
}
\end{lstlisting}

Furthermore, the "RealSenseConnector" class also defines all the methods that have their implementation in the support library. All these methods have equivalents in the native library that interact with the main loop function of the C++ code.

\begin{lstlisting}[language=Java,name={Java Native Interface methods},label={rs:javaplugin:4}]
// Native methods for recognition control
public native void start_recognition();  // Blocking call that starts recognition loop
public native void pause_recognition();  // Does the same as stop
public native void stop_recognition();  // Stops the recgonition loop
// Visualization - not implemented
public native void start_visualization();
public native void stop_visualization();
\end{lstlisting}

The visualization part is not implemented in C++ and not used by the Java plugin for that reason. The cause for this decision is explained later in the results chapter.

\subsubsection{Threading native code in Java}

Components of AsTeRICS need to be able to interact with the graphical user interface (GUI). For this reason, the main plugin class runs on the same thread as the GUI. This means that the hand recognition had to be done on a seperate thread. There are two sensible ways of approaching this problem.

The first way is to perform all threading inside the native C++ support library. This has been made significantly easier to do in C++ since the introduction of the "C++11" standard that is now widely supported by compilers. Nevertheless it is still harder to interact and share data between threads than in higher level languages such as Java. Additionally, this approach requires more interaction between Java and C++ in this case because the Java code needs more precise control of the threading.

The second way involves performing the threading within Java. The native functionality in the library is called from within a Java thread and executed there. This results in less direct control of the C++ code but is faster and less complex to implement.

Considering both ways, the second approach has considerably less overhead because it features fewer JNI functions. It is also faster and easier to implement due to the more advanced threading functionality in Java. For these reasons, it was the approach chosen for the implementation of the gesture recognition plugin.

\newpage
Start of the native recognition loop blocks execution of its thread while running. It is executed in a Java Thread implemented as the "RealSenseNativeConnectorThread" class. This class is initialized with the instance of the native connector thread class as "parent" and a start delay. This delay is sometimes used by the native connector when restarting the native loop after an error. In various instances during testing a random failure of the USB connection occurred and thus the delay is used to give the user some time to rectify the problem.


\begin{lstlisting}[language=Java,name={Functionality inside the native connector threading class},label={rs:javaplugin:6}]
// Do the blocking recognition loop in thread
try{
	if(startDelay > 0) { Thread.sleep(startDelay); }
	this.parent.start_recognition();
	System.out.println("[RealSenseGestures] Recognition loop stopped - Thread exiting");
}catch (Exception e){
	System.out.println("[RealSenseGestures] Error in recognition loop - stack trace below");
	e.printStackTrace();
	parent.recognitionThreadExited();
}
\end{lstlisting}

When the thread is started it starts the recognition loop inside the native library. This blocks thread execution until it is either stopped by the native connector class or runs into an error, in which case the catch block is triggered, the error reported and the parent notified. The parent will attempt to start a new thread after a delay.

\chapter{Result}
\section{Achieved functionality}

What follows is a list of all the components implemented as well as their respective functionality and limitations.

\begin{itemize}
	\item A native library for Windows and Linux in 64-bit architecture that can connect to an IntelRealSense camera and use its depth sensor to recognize a hand and the number of extended fingers using OpenCV. It also contains functionality for being controlled through a Java class and reporting information back to it through use of the Java Native Interface.
	\item A small test application used to test that library with a visualization of the recognition process.
	\item A Java AsTeRICS component plugin that runs the library in a seperate thread to recognize the number of fingers extended and relay that data to an AsTeRICS model.
\end{itemize}

\newpage

\section{Evaluation}
%TODO: Write about test asterics model, math limitations (hand turning, middle of the screen), show screen of model
\subsection{Fist mouse click}
\subsection{Number entry}


%TODO: Write conclusion, rate method effectiveness (Mistakes made), do advice (JNI practices, IDE), and list roadmap
\chapter{Conclusion}
\section{Effectiveness of method}

Analyzation of the AsTeRICS framework proved the first method to work very well. The three proposed tiers are easily distinguishable while being seperated enough to be of use for decision-making. Components of the framework being exclusively written as plugins following the OSGi architecture made this process more straightforward than could be the case in other applications, in which cases the layer definitions might need to be expanded.

Choosing technology for the gesture recognition component proved more complicated than predicted by the second method. An add-in might not want to settle on one technology - as an example, one programming language - for implementation. This is due to factors like library availability, which weren't considered in the method for complexity reasons. The failed Java implementation of the developed plugin is proof of this. An improvement for this method would be an easy way to define a collection of technologies that combine well (as was done with Java and C++ through the JNI), rather than a single one.

The attachment point is highly dependent on the result of the first two methods. Attaching to the OSGi layer was a prudent decision and further proved that the layer method works well. This would not have been so easily achieved if Java as the base technology of the plugin had to have been completely removed. As a result, a good improvement for the 3rd method would be a way to make the step of finding an attachment point more independent of the other two steps. A change of layer or technology should not affect the chosen attachment point as drastically as it does with the currently proposed method.

\section{Tips for AsTeRICS component development}



\section{Possibility of improvement}

%
% Hier beginnen die Verzeichnisse.
%
\clearpage
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{}{\bibliographystyle{ieee}}
\bibliography{Literatur}
\clearpage

% Das Abbildungsverzeichnis
\listoffigures
\clearpage

% Das Tabellenverzeichnis
\listoftables
\clearpage

% Das Quellcodeverzeichnis
\listofcode
\clearpage

\phantomsection
\addcontentsline{toc}{chapter}{\listacroname}
\chapter*{\listacroname}
\begin{acronym}[XXXXX]
	\acro{AT}[AT]{Assistive Technology}
    \acro{W3C}[W3C]{World Wide Web Consortium}
    \acro{JSON}[JSON]{JavaScript Object Notation}
    \acro{XML}[XML]{Extensible Markup Language}
    \acro{AsTeRICS}[AsTeRICS]{Assistive Technology Rapid Integration \& Construction Set}
    \acro{OpenCV}[OpenCV]{Open Computer Vision}
    \acro{IDE}[IDE]{Integrated Development Environment}
    \acro{REST}[REST]{Representational State Transfer}
    \acro{API}[API]{Application Programming Interface}
    \acro{HTTP}[HTTP]{Hypertext Transfer Protocol}
    \acro{OSGi}{OSGi}{Open Service Gateway Initiative}
    \acro{JNI}{JNI}{Java Native Interface}
    \acro{GUI}{GUI}{Graphical User Interface}
\end{acronym}

%
% Hier beginnt der Anhang.
%
\clearpage
\appendix
\chapter{Anhang A}
\clearpage
\chapter{Anhang B}
\end{document}